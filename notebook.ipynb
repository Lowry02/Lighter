{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lighter training paper"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setting DataLoader and Image Transformations\n",
    "In order to be sure that everything is installed and working properly, we are going to download the dataset and simulate an inference cycle."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from torchvision import datasets\n",
    "import torchvision.transforms.v2 as transforms\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.data import RandomSampler\n",
    "\n",
    "from Lighter import Lighter\n",
    "from utils.transformations import ToGraph, NoisyImage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# loading MNIST\n",
    "data_transforms = transforms.Compose([\n",
    "  transforms.ToTensor(),\n",
    "  transforms.Pad((0,0,2,2)),\n",
    "  transforms.RandomRotation(degrees=(0,180)),\n",
    "  NoisyImage(),\n",
    "  transforms.RandomInvert(p=0.5),\n",
    "  ToGraph()\n",
    "])\n",
    "\n",
    "train_dataset = datasets.MNIST(\n",
    "  root=\"./data\",\n",
    "  train=True,\n",
    "  download=True,\n",
    "  transform=data_transforms\n",
    ")\n",
    "train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=2, shuffle=True)\n",
    "\n",
    "test_dataset = datasets.MNIST(\n",
    "  root=\"./data\",\n",
    "  train=False,\n",
    "  download=True,\n",
    "  transform=data_transforms\n",
    ")\n",
    "test_loader = torch.utils.data.DataLoader(test_dataset, batch_size=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# selecting 10000 items for training and 1000 for testing\n",
    "train_indeces = torch.Tensor().int()\n",
    "test_indeces = torch.Tensor().int()\n",
    "train_items_per_target = 500\n",
    "test_items_per_target = 100\n",
    "\n",
    "for target in train_dataset.targets.unique():\n",
    "  target = target.item()\n",
    "  target_indeces = torch.nonzero(train_dataset.targets == target).reshape(-1)\n",
    "  train_indeces = torch.cat((train_indeces, target_indeces[:train_items_per_target]), dim=0)\n",
    "  target_indeces = torch.nonzero(test_dataset.targets == target).reshape(-1)\n",
    "  test_indeces = torch.cat((test_indeces, target_indeces[:test_items_per_target]), dim=0)\n",
    "\n",
    "train_indeces = train_indeces[torch.randperm(train_indeces.size(0))]\n",
    "test_indeces = test_indeces[torch.randperm(test_indeces.size(0))]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Lighter()\n",
    "\n",
    "for batch in train_loader:\n",
    "  with torch.no_grad():\n",
    "    (X,A), L = batch\n",
    "    output = model(X,A)\n",
    "    print(output.shape)\n",
    "    break\n",
    "  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train Loop\n",
    "We will use WandB to log the data obtained from the training process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import wandb\n",
    "import itertools\n",
    "import inspect"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wandb.login(relogin=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Â train loop settings\n",
    "log_enabled = False\n",
    "restore_run = log_enabled and False\n",
    "run_id = \"mltxwb9p\"\n",
    "learning_rate = 1e-03\n",
    "project_name = f\"Lighter 6.0\"\n",
    "batch_size = 64"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# device\n",
    "if torch.cuda.is_available():\n",
    "  device = torch.device(\"cuda\")\n",
    "else:\n",
    "  device = torch.device(\"cpu\")\n",
    "\n",
    "# dataloaders\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, sampler=RandomSampler(train_indeces))\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size,  sampler=RandomSampler(test_indeces))\n",
    "\n",
    "model = Lighter().to(device)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "cross_entropy = nn.CrossEntropyLoss()\n",
    "\n",
    "starting_epoch = 1\n",
    "\n",
    "# restoring last run status\n",
    "if restore_run:\n",
    "  wandb.init(\n",
    "    entity=\"lorenzocusin02\",\n",
    "    project=\"ML Project\",\n",
    "    id=run_id,\n",
    "    resume=\"must\"\n",
    "  )\n",
    "  api = wandb.Api()\n",
    "  run = api.run(f\"lorenzocusin02/ML Project/{run_id}\")\n",
    "  local_path = run.file(\"train_status.info\").download(replace=True)\n",
    "  print(\"> Model restored\")\n",
    "  checkpoint = torch.load(\"./train_status.info\", map_location=torch.device('cpu'))\n",
    "  model = model.to(device)\n",
    "  model.load_state_dict(checkpoint['model_state_dict'])\n",
    "  optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
    "  starting_epoch = checkpoint['epoch']\n",
    "elif log_enabled:\n",
    "  wandb.init(\n",
    "    project = \"ML Project\",\n",
    "    name = project_name,\n",
    "    notes = f\"\"\"\n",
    "        LEARNING RATE: {learning_rate}\n",
    "        class Lighter(nn.Module):\n",
    "          {inspect.getsource(Lighter.__init__)}\n",
    "          {inspect.getsource(Lighter.forward)}\n",
    "    \"\"\"\n",
    "  )\n",
    "  \n",
    "for param_group in optimizer.param_groups:\n",
    "  param_group['lr'] = learning_rate\n",
    "\n",
    "train_loss = []\n",
    "train_acc = []\n",
    "test_loss = []\n",
    "test_acc = []\n",
    "\n",
    "print(f\"> Device: {device}\")\n",
    "print(f\"> Learning rate: {learning_rate}\")\n",
    "\n",
    "for epoch in itertools.count(start=starting_epoch):\n",
    "  if (epoch % 10 == 0) and log_enabled:\n",
    "    print(\"> Saving training status\")\n",
    "    torch.save({\n",
    "        'epoch': epoch,\n",
    "        'model_state_dict': model.state_dict(),\n",
    "        'optimizer_state_dict': optimizer.state_dict(),\n",
    "      },\n",
    "      \"./train_status.info\"\n",
    "    )\n",
    "    wandb.save(\"./train_status.info\")\n",
    "\n",
    "  print(f\"> Epoch {epoch}\")\n",
    "\n",
    "  # training\n",
    "  model.train()\n",
    "  running_loss = 0\n",
    "  running_acc = 0\n",
    "  for i, batch in enumerate(train_loader):\n",
    "    (features, adjency_matrix), labels = batch\n",
    "    features = features.to(device)\n",
    "    adjency_matrix = adjency_matrix.float().to(device)\n",
    "    labels = labels.to(device)\n",
    "    output = model(features, adjency_matrix)\n",
    "    \n",
    "    loss = cross_entropy(output, labels)\n",
    "    \n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    running_loss += loss.item()\n",
    "    running_acc += (torch.max(output, 1).indices == labels).sum().item() / labels.shape[0]\n",
    "\n",
    "  train_loss.append(running_loss / (i + 1))\n",
    "  train_acc.append(running_acc / (i + 1) * 100)\n",
    "\n",
    "  print(f\"\\tTrain Loss: {train_loss[-1]}\")\n",
    "  print(f\"\\tTrain Accuracy: {train_acc[-1]}\")\n",
    "\n",
    "  # evaluation\n",
    "  model.eval()\n",
    "  running_loss = 0\n",
    "  running_acc = 0\n",
    "  with torch.no_grad():\n",
    "    for i, batch in enumerate(test_loader):\n",
    "      (features, adjency_matrix), labels = batch\n",
    "      features = features.to(device)\n",
    "      adjency_matrix = adjency_matrix.float().to(device)\n",
    "      labels = labels.to(device)\n",
    "    \n",
    "      output = model(features, adjency_matrix)\n",
    "      \n",
    "      loss = cross_entropy(output, labels)\n",
    "    \n",
    "      running_loss += loss.item()\n",
    "      running_acc += (torch.max(output, 1).indices == labels).sum().item() / labels.shape[0]\n",
    "\n",
    "    test_loss.append(running_loss / (i + 1))\n",
    "    test_acc.append(running_acc / (i + 1) * 100)\n",
    "\n",
    "  print(f\"\\tTest Loss: {test_loss[-1]}\")\n",
    "  print(f\"\\tTest Accuracy: {test_acc[-1]}\")\n",
    "  \n",
    "  print()\n",
    "  \n",
    "  if log_enabled:\n",
    "    wandb.log({\n",
    "      \"train_loss\" : train_loss[-1],\n",
    "      \"train_acc\" : train_acc[-1],\n",
    "      \"validation_loss\" : test_loss[-1],\n",
    "      \"validation_acc\" : test_acc[-1],\n",
    "    })\n",
    "\n",
    "  if (epoch % 10 == 0) and log_enabled:\n",
    "    print(\"> Saving training status\")\n",
    "    torch.save({\n",
    "        'epoch': epoch,\n",
    "        'model_state_dict': model.state_dict(),\n",
    "        'optimizer_state_dict': optimizer.state_dict(),\n",
    "      },\n",
    "      \"./train_status.info\"\n",
    "    )\n",
    "    wandb.save(\"./train_status.info\")\n",
    "\n",
    "if log_enabled:\n",
    "  wandb.finish()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 828,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save({\n",
    "    'epoch': epoch,\n",
    "    'model_state_dict': model.state_dict(),\n",
    "    'optimizer_state_dict': optimizer.state_dict(),\n",
    "  },\n",
    "  \"./train_status.info\"\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
